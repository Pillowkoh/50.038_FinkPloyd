{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "    \n",
    "my_features_path = 'deam-dataset/my_features/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile as zf\n",
    "import os\n",
    "\n",
    "os.chdir(my_features_path)\n",
    "files = zf.ZipFile(\"mfcc.zip\", 'r')\n",
    "files.extractall('mfcc/')\n",
    "files.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting zipfile38\n",
      "  Downloading zipfile38-0.0.3.tar.gz (22 kB)\n",
      "Building wheels for collected packages: zipfile38\n",
      "  Building wheel for zipfile38 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for zipfile38: filename=zipfile38-0.0.3-py3-none-any.whl size=22765 sha256=b2f67b85ac46631b2adf7c931fcfdcff85338fed52e27609496065c5f18f0fb3\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/32/e9/b6/cf61548b40caf6cf2a84bb123064993db2215cc8c13f23f58e\n",
      "Successfully built zipfile38\n",
      "Installing collected packages: zipfile38\n",
      "Successfully installed zipfile38-0.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install zipfile38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile38 as zf38\n",
    "files = zf38.ZipFile(\"mel_spectogram_features.zip\", 'r')\n",
    "files.extractall()\n",
    "files.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best_model2',\n",
       " 'deam-dataset',\n",
       " 'valid_data.npz',\n",
       " 'deam_data_fetch.py',\n",
       " 'best_model',\n",
       " 'deam_cnn_processing.ipynb',\n",
       " '.Trash-1000',\n",
       " 'train_data.npz',\n",
       " 'deam_cnn.py',\n",
       " 'test_cnn_log.txt',\n",
       " 'test_data.npz',\n",
       " 'deam_cnn_model.ipynb',\n",
       " 'DEAM_CNN-LSTM.ipynb',\n",
       " '.ipynb_checkpoints',\n",
       " 'deam-dataset.zip',\n",
       " '__pycache__']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('..')\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1802, 1920, 20)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fetch_mfcc(dir):\n",
    "    files = os.listdir(dir)\n",
    "    csv_files = list(filter(lambda f: '.csv' in f, files))\n",
    "    mfcc_files = sorted(csv_files, key=lambda f: int(f[5:][:-4]))   # sort by audio index (remove 'mfcc' and '.csv')\n",
    "    mfcc_data = np.empty(shape=(len(mfcc_files), 1920, 20), dtype=np.float32)\n",
    "    for i, f in enumerate(mfcc_files):\n",
    "        if '.csv' not in f: continue \n",
    "        p = os.path.join(dir, f)\n",
    "        mfcc = np.loadtxt(p, delimiter=',')\n",
    "        truncated_mfcc = mfcc[:, :1920]\n",
    "        mfcc_data[i] = truncated_mfcc.T\n",
    "    return mfcc_data\n",
    "\n",
    "my_features_path = 'deam-dataset/my_features/'\n",
    "mfcc_path = os.path.join(my_features_path, 'mfcc')\n",
    "mfcc_data = fetch_mfcc(mfcc_path)\n",
    "mfcc_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1802, 1920, 128)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fetch_mel_spectograms(dir):\n",
    "    files = os.listdir(dir)\n",
    "    csv_files = list(filter(lambda f: '.csv' in f, files))\n",
    "    mel_files = sorted(csv_files, key=lambda f: int(f[len('mel_spectogram_'):][:-4]))   # sort by audio index (remove 'mel_spectogram_' and '.csv')\n",
    "    mel_spec_data = np.empty(shape=(len(mel_files), 1920, 128, ), dtype=np.float32)\n",
    "\n",
    "    for i, f in enumerate(mel_files):\n",
    "        p = os.path.join(dir, f)\n",
    "        mel_spec = np.loadtxt(p, delimiter=',', dtype=np.float32)\n",
    "        truncated_mel_spec = mel_spec[:, :1920]\n",
    "        mel_spec_data[i] = truncated_mel_spec.T\n",
    "    return mel_spec_data\n",
    "\n",
    "mel_path = os.path.join(my_features_path, 'mel_spectogram_features')\n",
    "mel_spec_data = fetch_mel_spectograms(mel_path)\n",
    "mel_spec_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('mfcc_data', mfcc_data)\n",
    "np.save('mel_spectogram_data', mel_spec_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_id</th>\n",
       "      <th>valence_mean</th>\n",
       "      <th>valence_std</th>\n",
       "      <th>arousal_mean</th>\n",
       "      <th>arousal_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>3.10</td>\n",
       "      <td>0.94</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>3.50</td>\n",
       "      <td>1.75</td>\n",
       "      <td>3.30</td>\n",
       "      <td>1.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>5.70</td>\n",
       "      <td>1.42</td>\n",
       "      <td>5.50</td>\n",
       "      <td>1.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>4.40</td>\n",
       "      <td>2.01</td>\n",
       "      <td>5.30</td>\n",
       "      <td>1.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>5.80</td>\n",
       "      <td>1.47</td>\n",
       "      <td>6.40</td>\n",
       "      <td>1.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>2054</td>\n",
       "      <td>5.40</td>\n",
       "      <td>1.20</td>\n",
       "      <td>3.60</td>\n",
       "      <td>1.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>2055</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.41</td>\n",
       "      <td>5.20</td>\n",
       "      <td>1.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>2056</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.41</td>\n",
       "      <td>4.60</td>\n",
       "      <td>1.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>2057</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1.07</td>\n",
       "      <td>6.83</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1801</th>\n",
       "      <td>2058</td>\n",
       "      <td>3.80</td>\n",
       "      <td>0.75</td>\n",
       "      <td>5.80</td>\n",
       "      <td>1.17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1802 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      song_id  valence_mean  valence_std  arousal_mean  arousal_std\n",
       "0           2          3.10         0.94          3.00         0.63\n",
       "1           3          3.50         1.75          3.30         1.62\n",
       "2           4          5.70         1.42          5.50         1.63\n",
       "3           5          4.40         2.01          5.30         1.85\n",
       "4           7          5.80         1.47          6.40         1.69\n",
       "...       ...           ...          ...           ...          ...\n",
       "1797     2054          5.40         1.20          3.60         1.36\n",
       "1798     2055          5.00         1.41          5.20         1.47\n",
       "1799     2056          5.00         1.41          4.60         1.74\n",
       "1800     2057          3.17         1.07          6.83         0.37\n",
       "1801     2058          3.80         0.75          5.80         1.17\n",
       "\n",
       "[1802 rows x 5 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fetch labels\n",
    "label_path = 'deam-dataset/DEAM_Annotations/annotations/annotations averaged per song/song_level/';\n",
    "labels_1_2000 = pd.read_csv(os.path.join(label_path, 'static_annotations_averaged_songs_1_2000.csv'))\n",
    "labels_2000_2058 = pd.read_csv(os.path.join(label_path,'static_annotations_averaged_songs_2000_2058.csv'))\n",
    "labels = pd.concat([labels_1_2000, labels_2000_2058], ignore_index=True, sort=False)\n",
    "labels = labels[labels_1_2000.columns]\n",
    "labels.columns = labels.columns.str.replace(' ', '')\n",
    "\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1802, 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valence = labels['valence_mean'].to_numpy()\n",
    "arousal = labels['arousal_mean'].to_numpy()\n",
    "y = np.vstack([valence, arousal]).T\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated input shape:  (1802, 1920, 148)\n"
     ]
    }
   ],
   "source": [
    "concat_input = np.concatenate((mfcc_data, mel_spec_data), 2)\n",
    "print('Concatenated input shape: ', concat_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_ids = list(range(concat_input.shape[0]))\n",
    "\n",
    "X_trn_ids, X_test_ids, y_trn, y_test = train_test_split(x_ids, y, test_size=0.2, random_state=23)\n",
    "X_train_ids, X_valid_ids, y_train, y_valid = train_test_split(X_trn_ids, y_trn, test_size=0.33,  random_state=23)\n",
    "data_size = concat_input.shape[0]\n",
    "X_train, X_valid, X_test = concat_input[X_train_ids], concat_input[X_valid_ids], concat_input[X_test_ids]\n",
    "# split_1, split_2 = (int) (0.6 * data_size), (int) (0.8 * data_size)\n",
    "# X_train, X_valid, X_test = concat_input[:split_1], concat_input[split_1:split_2], concat_input[split_2:]\n",
    "# y_train, y_valid, y_test = y[:split_1], y[split_1:split_2], y[split_2:]\n",
    "\n",
    "X_mfcc_train, X_mel_train = X_train[:, :, :20], X_train[:, :, 20:]\n",
    "X_mfcc_valid, X_mel_valid = X_valid[:, :, :20], X_valid[:, :, 20:]\n",
    "X_mfcc_test, X_mel_test = X_test[:, :, :20], X_test[:, :, 20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('cnn_train_data', x=X_train, y=y_train)\n",
    "np.savez_compressed('cnn_valid_data', x=X_valid, y=y_valid)\n",
    "np.savez_compressed('cnn_test_data', x=X_test, y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, LayerNormalization, BatchNormalization\n",
    "\n",
    "# drop 0th coefficient as it only conveys a constant offset\n",
    "X_mfcc_train_f = np.delete(X_mfcc_train, 0, axis=2)\n",
    "X_mfcc_valid_f = np.delete(X_mfcc_valid, 0, axis=2)\n",
    "X_mfcc_test_f = np.delete(X_mfcc_test, 0, axis=2)\n",
    "conv_layer = partial(Conv1D, kernel_size=4, activation='relu', padding='SAME', strides=1)\n",
    "pooling = partial(MaxPooling1D, pool_size=4, strides=4, padding='SAME')\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    LayerNormalization(),\n",
    "    conv_layer(input_shape=(1920, 19), filters=32, kernel_size=6),\n",
    "    pooling(),\n",
    "    conv_layer(filters=64),\n",
    "    pooling(),\n",
    "    conv_layer(filters=128),\n",
    "    pooling(),\n",
    "    Flatten(),\n",
    "    Dense(units=128, activation='relu'),\n",
    "    Dropout(0.4),\n",
    "    Dense(units=2)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "60/61 [============================>.] - ETA: 0s - loss: 3.9123 - mean_squared_error: 17.7947INFO:tensorflow:Assets written to: best_model2/assets\n",
      "61/61 [==============================] - 2s 25ms/step - loss: 3.9076 - mean_squared_error: 17.7579 - val_loss: 2.9639 - val_mean_squared_error: 10.6031\n",
      "Epoch 2/30\n",
      "49/61 [=======================>......] - ETA: 0s - loss: 2.1481 - mean_squared_error: 6.5700INFO:tensorflow:Assets written to: best_model2/assets\n",
      "61/61 [==============================] - 2s 27ms/step - loss: 2.0357 - mean_squared_error: 6.0207 - val_loss: 1.4384 - val_mean_squared_error: 2.9809\n",
      "Epoch 3/30\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.4790 - mean_squared_error: 3.3778INFO:tensorflow:Assets written to: best_model2/assets\n",
      "61/61 [==============================] - 1s 24ms/step - loss: 1.4790 - mean_squared_error: 3.3778 - val_loss: 1.2200 - val_mean_squared_error: 2.2337\n",
      "Epoch 4/30\n",
      "51/61 [========================>.....] - ETA: 0s - loss: 1.4591 - mean_squared_error: 3.2037INFO:tensorflow:Assets written to: best_model2/assets\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 1.4458 - mean_squared_error: 3.1556 - val_loss: 1.1866 - val_mean_squared_error: 2.1154\n",
      "Epoch 5/30\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.4016 - mean_squared_error: 2.9699INFO:tensorflow:Assets written to: best_model2/assets\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 1.4016 - mean_squared_error: 2.9699 - val_loss: 1.1577 - val_mean_squared_error: 2.0144\n",
      "Epoch 6/30\n",
      "49/61 [=======================>......] - ETA: 0s - loss: 1.3775 - mean_squared_error: 2.8972INFO:tensorflow:Assets written to: best_model2/assets\n",
      "61/61 [==============================] - 2s 26ms/step - loss: 1.3814 - mean_squared_error: 2.9061 - val_loss: 1.1173 - val_mean_squared_error: 1.8918\n",
      "Epoch 7/30\n",
      "60/61 [============================>.] - ETA: 0s - loss: 1.3779 - mean_squared_error: 2.9220INFO:tensorflow:Assets written to: best_model2/assets\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 1.3788 - mean_squared_error: 2.9250 - val_loss: 1.0835 - val_mean_squared_error: 1.7906\n",
      "Epoch 8/30\n",
      "61/61 [==============================] - 0s 7ms/step - loss: 1.3765 - mean_squared_error: 2.8841 - val_loss: 1.0969 - val_mean_squared_error: 1.8068\n",
      "Epoch 9/30\n",
      "54/61 [=========================>....] - ETA: 0s - loss: 1.3320 - mean_squared_error: 2.6974INFO:tensorflow:Assets written to: best_model2/assets\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 1.3418 - mean_squared_error: 2.7157 - val_loss: 1.0671 - val_mean_squared_error: 1.7267\n",
      "Epoch 10/30\n",
      "61/61 [==============================] - 0s 7ms/step - loss: 1.3254 - mean_squared_error: 2.6966 - val_loss: 1.0826 - val_mean_squared_error: 1.7489\n",
      "Epoch 11/30\n",
      "61/61 [==============================] - 0s 7ms/step - loss: 1.3296 - mean_squared_error: 2.6590 - val_loss: 1.0783 - val_mean_squared_error: 1.7292\n",
      "Epoch 12/30\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.2937 - mean_squared_error: 2.5803INFO:tensorflow:Assets written to: best_model2/assets\n",
      "61/61 [==============================] - 2s 28ms/step - loss: 1.2937 - mean_squared_error: 2.5803 - val_loss: 1.0256 - val_mean_squared_error: 1.6042\n",
      "Epoch 13/30\n",
      "61/61 [==============================] - 0s 7ms/step - loss: 1.3260 - mean_squared_error: 2.6830 - val_loss: 1.0441 - val_mean_squared_error: 1.6318\n",
      "Epoch 14/30\n",
      "61/61 [==============================] - 0s 7ms/step - loss: 1.2877 - mean_squared_error: 2.5082 - val_loss: 1.0321 - val_mean_squared_error: 1.5914\n",
      "Epoch 15/30\n",
      "61/61 [==============================] - 0s 7ms/step - loss: 1.2748 - mean_squared_error: 2.5160 - val_loss: 1.0433 - val_mean_squared_error: 1.6322\n",
      "Epoch 16/30\n",
      "61/61 [==============================] - 0s 7ms/step - loss: 1.3287 - mean_squared_error: 2.6870 - val_loss: 1.0459 - val_mean_squared_error: 1.6334\n",
      "Epoch 17/30\n",
      "54/61 [=========================>....] - ETA: 0s - loss: 1.2503 - mean_squared_error: 2.3681INFO:tensorflow:Assets written to: best_model2/assets\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 1.2644 - mean_squared_error: 2.4257 - val_loss: 1.0166 - val_mean_squared_error: 1.5563\n",
      "Epoch 18/30\n",
      "60/61 [============================>.] - ETA: 0s - loss: 1.2476 - mean_squared_error: 2.4154INFO:tensorflow:Assets written to: best_model2/assets\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 1.2504 - mean_squared_error: 2.4267 - val_loss: 1.0065 - val_mean_squared_error: 1.5232\n",
      "Epoch 19/30\n",
      "61/61 [==============================] - 0s 7ms/step - loss: 1.2930 - mean_squared_error: 2.5322 - val_loss: 1.0342 - val_mean_squared_error: 1.5846\n",
      "Epoch 20/30\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 1.2710 - mean_squared_error: 2.5055 - val_loss: 1.0072 - val_mean_squared_error: 1.5266\n",
      "Epoch 21/30\n",
      "61/61 [==============================] - 0s 7ms/step - loss: 1.2782 - mean_squared_error: 2.4910 - val_loss: 1.0336 - val_mean_squared_error: 1.5857\n",
      "Epoch 22/30\n",
      "61/61 [==============================] - 0s 7ms/step - loss: 1.2736 - mean_squared_error: 2.4379 - val_loss: 1.0133 - val_mean_squared_error: 1.5299\n",
      "Epoch 23/30\n",
      "61/61 [==============================] - 0s 7ms/step - loss: 1.2275 - mean_squared_error: 2.3222 - val_loss: 1.0318 - val_mean_squared_error: 1.5700\n",
      "Epoch 24/30\n",
      "57/61 [===========================>..] - ETA: 0s - loss: 1.2343 - mean_squared_error: 2.3620INFO:tensorflow:Assets written to: best_model2/assets\n",
      "61/61 [==============================] - 1s 22ms/step - loss: 1.2342 - mean_squared_error: 2.3621 - val_loss: 1.0053 - val_mean_squared_error: 1.5196\n",
      "Epoch 25/30\n",
      "61/61 [==============================] - 0s 7ms/step - loss: 1.2737 - mean_squared_error: 2.4392 - val_loss: 1.0078 - val_mean_squared_error: 1.5182\n",
      "Epoch 26/30\n",
      "61/61 [==============================] - 0s 7ms/step - loss: 1.2898 - mean_squared_error: 2.4975 - val_loss: 1.0218 - val_mean_squared_error: 1.5501\n",
      "Epoch 27/30\n",
      "61/61 [==============================] - 0s 7ms/step - loss: 1.2790 - mean_squared_error: 2.4725 - val_loss: 1.0087 - val_mean_squared_error: 1.5098\n",
      "Epoch 28/30\n",
      "61/61 [==============================] - 0s 7ms/step - loss: 1.2481 - mean_squared_error: 2.3536 - val_loss: 1.0210 - val_mean_squared_error: 1.5375\n"
     ]
    }
   ],
   "source": [
    "model_path = 'best_model2/'\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(monitor='val_loss', save_best_only=True, filepath=model_path)\n",
    "\n",
    "optimizer = keras.optimizers.Adam(lr=0.00001)\n",
    "model.compile(loss='mae', optimizer=optimizer, metrics=['mean_squared_error'])\n",
    "history = model.fit(X_mfcc_train_f, y_train, \n",
    "                    epochs=30,\n",
    "                    validation_data=(X_mfcc_valid_f, y_valid), \n",
    "                    batch_size=16, \n",
    "                    callbacks=[early_stopping, checkpoint])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 5ms/step - loss: 0.9896 - mean_squared_error: 1.4743\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_mfcc_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_val_arousal(data):\n",
    "    cat_data = []\n",
    "    # A: active, P: passive\n",
    "    # P: positive, N: negative\n",
    "    for point in data:\n",
    "        valence, arousal = point[0], point[1]\n",
    "        if valence >= 5 and arousal >= 5:\n",
    "            cat_data.append('AP')\n",
    "        elif valence < 5 and arousal >= 5:\n",
    "            cat_data.append('AN')\n",
    "        elif valence >= 5 and arousal < 5:\n",
    "            cat_data.append('PP')\n",
    "        elif valence < 5 and arousal < 5:\n",
    "            cat_data.append('PN')\n",
    "    return np.array(cat_data, dtype='str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.3518005540166205\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "y_pred = model.predict(X_mfcc_test_f)\n",
    "\n",
    "cat_y_pred = categorize_val_arousal(y_pred)\n",
    "cat_y_test = categorize_val_arousal(y_test)\n",
    "\n",
    "accuracy = accuracy_score(cat_y_test, cat_y_pred)\n",
    "print(f'Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.75282  , 4.8825507], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.2, 3.6])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_model = keras.models.Sequential([\n",
    "    LayerNormalization(),\n",
    "    conv_layer(input_shape=(1920, 128), filters=32, kernel_size=8),\n",
    "    pooling(),\n",
    "    conv_layer(filters=64),\n",
    "    pooling(),\n",
    "    conv_layer(filters=128),\n",
    "    pooling(),\n",
    "    Flatten(),\n",
    "    Dense(units=128, activation='relu'),\n",
    "    Dropout(0.4),\n",
    "    Dense(units=2)\n",
    "])\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n",
    "\n",
    "optimizer = keras.optimizers.Adam(lr=0.00001)\n",
    "mel_model.compile(loss='mae', optimizer=optimizer, metrics=['mean_squared_error'])\n",
    "history = mel_model.fit(X_mel_train, y_train, \n",
    "                    epochs=30,\n",
    "                    validation_data=(X_mel_valid, y_valid), \n",
    "                    batch_size=8, \n",
    "                    callbacks=[early_stopping])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_y = np.array(categorize_val_arousal(y))\n",
    "\n",
    "cat_y_train, cat_y_valid, cat_y_test = categorize_val_arousal(y_train), categorize_val_arousal(y_valid), categorize_val_arousal(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mel_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d4001fd22ea6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model = keras.models.Sequential([\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mLayerNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mconv_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1920\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpooling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mconv_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    LayerNormalization(axis=1),\n",
    "    conv_layer(input_shape=(1920, 20), filters=64, kernel_size=8),\n",
    "    pooling(),\n",
    "    conv_layer(filters=64),\n",
    "    pooling(),\n",
    "    conv_layer(filters=128),\n",
    "    pooling(),\n",
    "    Flatten(),\n",
    "    Dense(units=256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(units=1, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(X_mfcc_train, cat_y_train, \n",
    "                    epochs=30,\n",
    "                    validation_data=(X_mfcc_valid, cat_y_valid), \n",
    "                    batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
